{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "model1 =fasttext.skipgram(\"train_dl.txt\", \"dl_fasttext_model_1\",dim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import time\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "from pyfasttext import FastText\n",
    "import fasttext\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        # self.model = FastText(\"../data/input/models/sg_pyfasttext.bin\")  # DEBUG\n",
    "        # model1 =fasttext.skipgram(\"train.txt\", \"model\",dim = 50)\n",
    "        self.model = FastText(\"dl_fasttext_model_1.bin\")\n",
    "        # self.vocab_size, self.vector_size = self.model.numpy_normalized_vectors.shape  # OK\n",
    "        self.vocab_size = self.model.nwords\n",
    "        self.vector_size = self.model.args.get(\"dim\")\n",
    "        # self.vector_size:200, self.vocab_size: 925242\n",
    "        # print(f\"self.vector_size:{self.vector_size}, self.vocab_size: {self.vocab_size}\")\n",
    "\n",
    "        # 句子的表示形式:\n",
    "        # {\"avg\": 向量和的平均, \"fasttext\": get_numpy_sentence_vector, \"concatenate\": 向量拼接和补齐, \"matrix\": 矩阵}\n",
    "        self.sentence_vec_type = \"matrix\"\n",
    "\n",
    "        self.MAX_SENT_LEN = 70  # DEBUG: 超参数. self.get_sent_max_length()\n",
    "        # 对于\"concatenate\": self.MAX_SENT_LEN = 30, 取其他不同值的结果: 100: 50.22%, 80: 50.23%, 70: 50.33%, 60: 55.92%, 50: 69.11%, 40: 68.91%, 36: 69.34%, 30: 69.22%, 20: 69.17%, 10: 67.07%\n",
    "        # 对于\"matrix\": self.MAX_SENT_LEN = 70, 取其他不同值的结果: TODO:\n",
    "\n",
    "    @classmethod\n",
    "    def data_analysis(cls):\n",
    "        train_df = pd.read_csv(\"train_dl.txt\", sep=\"__label__\", header=None, names=[\"sentence\",\"label\"])\n",
    "        val_df = pd.read_csv(\"test_dl.txt\", sep=\"__label__\", header=None, names=[\"sentence\",\"label\"])\n",
    "        y_train = train_df[\"label\"]\n",
    "        y_val = val_df[\"label\"]\n",
    "        sns.set(style=\"white\", context=\"notebook\", palette=\"deep\")\n",
    "        # 查看样本数据分布情况(各个label数据是否均匀分布)\n",
    "        sns.countplot(y_train)\n",
    "        plt.show()\n",
    "        sns.countplot(y_val)\n",
    "        plt.show()\n",
    "        print(y_train.value_counts())\n",
    "        print(y_val.value_counts())\n",
    "\n",
    "    def set_sent_vec_type(self, sentence_vec_type):\n",
    "        assert sentence_vec_type in [\"avg\", \"concatenate\", \"fasttext\", \"matrix\"], \\\n",
    "            \"sentence_vec_type must be in ['avg', 'fasttext', 'concatenate', 'matrix']\"\n",
    "        self.sentence_vec_type = sentence_vec_type\n",
    "\n",
    "    def get_sent_max_length(self):  # NOT_USED\n",
    "        sent_len_counter = Counter()\n",
    "        max_length = 0\n",
    "        with open(\"../data/input/training_set.txt\") as f:\n",
    "            for line in f:\n",
    "                content = line.strip().split(\"\\t\")[1]\n",
    "                content_list = content.split()\n",
    "                length = len(content_list)\n",
    "                sent_len_counter[length] += 1\n",
    "                if max_length <= length:\n",
    "                    max_length = length\n",
    "        sent_len_counter = sorted(list(sent_len_counter.items()), key=lambda x: x[0])\n",
    "        print(sent_len_counter)\n",
    "        # [(31, 1145), (32, 1105), (33, 1017), (34, 938), (35, 839), (36, 830), (37, 775), (38, 737), (39, 720), (40, 643), (41, 575), (42, 584), (43, 517), (44, 547), (45, 514), (46, 514), (47, 480), (48, 460), (49, 470), (50, 444), (51, 484), (52, 432), (53, 462), (54, 495), (55, 487), (56, 500), (57, 496), (58, 489), (59, 419), (60, 387), (61, 348), (62, 265), (63, 222), (64, 153), (65, 127), (66, 103), (67, 67), (68, 34), (69, 21), (70, 22), (71, 8), (72, 6), (73, 4), (74, 10), (75, 2), (76, 4), (77, 2), (78, 1), (79, 2), (80, 4), (81, 2), (82, 3), (83, 1), (84, 5), (86, 4), (87, 3), (88, 3), (89, 2), (90, 2), (91, 3), (92, 5), (93, 2), (94, 4), (96, 1), (97, 5), (98, 1), (99, 2), (100, 2), (101, 2), (102, 1), (103, 2), (104, 2), (105, 2), (106, 5), (107, 3), (108, 2), (109, 3), (110, 4), (111, 1), (112, 2), (113, 3), (114, 1), (116, 1), (119, 3), (679, 1)]\n",
    "        return max_length\n",
    "\n",
    "    def gen_sentence_vec(self, sentence):\n",
    "        \"\"\"\n",
    "        :param sentence: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        sentence = sentence.strip()\n",
    "        if self.sentence_vec_type == \"fasttext\":\n",
    "            return self.model.get_numpy_sentence_vector(sentence)\n",
    "\n",
    "        word_list = sentence.split(\" \")\n",
    "        if self.sentence_vec_type == \"concatenate\":\n",
    "            sentence_vector = self.model.get_numpy_vector(word_list[0])\n",
    "            for word in word_list[1:]:\n",
    "                sentence_vector = np.hstack((sentence_vector, self.model.get_numpy_vector(word)))\n",
    "            return sentence_vector  # NOTE: 对于concatenate情况, 每个句子的sentence_vector是不一样长的\n",
    "        if self.sentence_vec_type == \"matrix\":  # for Deep Learning.\n",
    "            sentence_matrix = []\n",
    "            for word in word_list[-self.MAX_SENT_LEN:]:  # NOTE: 截取后面的应该是要好些(参考https://github.com/lxw0109/SentimentClassification_UMICH_SI650/blob/master/src/LSTM_wo_pretrained_vector.py#L86)\n",
    "                sentence_matrix.append(self.model.get_numpy_vector(word))\n",
    "            length = len(sentence_matrix)\n",
    "            # 一定成立，因为上面做了切片截取\n",
    "            assert length <= self.MAX_SENT_LEN, \"CRITICAL ERROR: len(sentence_matrix) > self.MAX_SENT_LEN.\"\n",
    "            # 参数中的matrix类型为list of ndarray, 返回值的matrix是ndarray of ndarray\n",
    "            sentence_matrix = np.pad(sentence_matrix, pad_width=((0, self.MAX_SENT_LEN - length), (0, 0)),\n",
    "                                     mode=\"constant\", constant_values=-1)\n",
    "            return sentence_matrix\n",
    "        else:  # self.sentence_vec_type == \"avg\":\n",
    "            sentence_vector = np.zeros(self.vector_size)  # <ndarray>\n",
    "            # print(f\"type(sentence_vector): {type(sentence_vector)}\")\n",
    "            for idx, word in enumerate(word_list):\n",
    "                # print(f\"type(self.model.get_numpy_vector(word)): {type(self.model.get_numpy_vector(word))}\")  # <ndarray>\n",
    "                sentence_vector += self.model.get_numpy_vector(word)\n",
    "            return sentence_vector / len(word_list)\n",
    "\n",
    "    def gen_train_val_data(self):\n",
    "        # 构造训练数据 & 验证数据\n",
    "        train_df = pd.read_csv(\"train_dl.txt\", sep=\"__label__\", header=None, names=[\"sentence\",\"label\"])\n",
    "        val_df = pd.read_csv(\"test_dl.txt\", sep=\"__label__\", header=None, names=[\"sentence\",\"label\"])\n",
    "        # 打乱训练集的顺序. TODO: 不打乱感觉训练出来的模型是有问题的?(好看那句总是预测结果是1？)\n",
    "        train_df = train_df.sample(frac=1, random_state=1)\n",
    "        # val_df = val_df.sample(frac=1, random_state=1)  # 验证集不用打乱\n",
    "\n",
    "        X_train = train_df[\"sentence\"]\n",
    "        X_train_vec = list()\n",
    "        for sentence in X_train:\n",
    "            sent_vector = self.gen_sentence_vec(sentence)\n",
    "            X_train_vec.append(sent_vector)\n",
    "        y_train = train_df[\"label\"]  # <Series>\n",
    "\n",
    "        X_val = val_df[\"sentence\"]\n",
    "        X_val_vec = list()\n",
    "        for sentence in X_val:\n",
    "            sent_vector = self.gen_sentence_vec(sentence)\n",
    "            X_val_vec.append(sent_vector)\n",
    "        y_val = val_df[\"label\"]  # <Series>\n",
    "\n",
    "        if self.sentence_vec_type == \"concatenate\":\n",
    "            # NOTE: 注意，这里的dtype是必须的，否则dtype默认值是\"int32\", 词向量所有的数值会被全部转换为0\n",
    "            X_train_vec = sequence.pad_sequences(X_train_vec, maxlen=self.MAX_SENT_LEN * self.vector_size, value=0,\n",
    "                                             dtype=np.float)\n",
    "            X_val_vec = sequence.pad_sequences(X_val_vec, maxlen=self.MAX_SENT_LEN * self.vector_size, value=0,\n",
    "                                           dtype=np.float)\n",
    "\n",
    "        return np.array(X_train_vec), np.array(X_val_vec), np.array(y_train), np.array(y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:101: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:102: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.set_sent_vec_type(\"fasttext\")\n",
    "\"\"\"\n",
    "    preprocess_obj.get_sent_max_length()\n",
    "    sentence = \"刘晓伟 好人\"  # gen_sentence_vec()函数里\"fasttext\"的情况感觉也得处理成这种情况(空格分格)?\n",
    "    # sentence = \"刘晓伟好人\"  # NOTE: 与空格分割得到的向量不同\n",
    "    preprocess_obj.set_sent_vec_type(\"fasttext\")\n",
    "    print(f'fasttext: {preprocess_obj.gen_sentence_vec(sentence)}')\n",
    "    preprocess_obj.set_sent_vec_type(\"avg\")\n",
    "    print(f'avg: {preprocess_obj.gen_sentence_vec(sentence)}')\n",
    "    preprocess_obj.set_sent_vec_type(\"concatenate\")\n",
    "    print(f'concatenate: {preprocess_obj.gen_sentence_vec(sentence)}')\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_val, y_train, y_val = preprocess_obj.gen_train_val_data()\n",
    "# print(f\"X_train: {X_train}\\nX_val: {X_val}\\ny_train: {y_train}\\ny_val: {y_val}\")\n",
    "# print(f\"X_train.shape: {X_train.shape}\\nX_val.shape: {X_val.shape}\\n\"\n",
    "#          f\"y_train.shape: {y_train.shape}\\ny_val.shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7c274ac1ce25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Masking\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self, preprocess_obj, sent_vec_type):\n",
    "        self.model_path_prefix=\"../data/output/models/\"\n",
    "        self.algorithm_name = \"nn\"\n",
    "        self.model_path = \"{self.model_path_prefix}{self.algorithm_name}_{sent_vec_type}\"\n",
    "        self.preprocess_obj = preprocess_obj\n",
    "        self.sent_vec_type = sent_vec_type\n",
    "        self.bath_size = 512  # TODO\n",
    "        self.epochs = 1000  # TODO\n",
    "\n",
    "    def pick_algorithm(self, algorithm_name, sent_vec_type):\n",
    "        assert algorithm_name in [\"nn\", \"cnn\", \"lstm\"], \"algorithm_name must be in ['nn', 'cnn', 'lstm']\"\n",
    "        self.algorithm_name = algorithm_name\n",
    "        self.model_path = \"{self.model_path_prefix}{self.algorithm_name}_{sent_vec_type}\"\n",
    "        self.sent_vec_type = sent_vec_type\n",
    "\n",
    "    def model_build(self, input_shape):\n",
    "        model_cls = Sequential()\n",
    "        if self.algorithm_name == \"nn\":  # Neural Network(Multi-Layer Perceptron)\n",
    "            # activation is essential for Dense, otherwise linear is used.\n",
    "            model_cls.add(Dense(64, input_shape=input_shape, activation=\"relu\", name=\"dense1\"))\n",
    "            model_cls.add(Dropout(0.25, name=\"dropout2\"))\n",
    "            model_cls.add(Dense(64, activation=\"relu\", name=\"dense3\"))\n",
    "            model_cls.add(Dropout(0.25, name=\"dropout4\"))\n",
    "            model_cls.add(Dense(2, activation=\"softmax\", name=\"dense5\"))\n",
    "            # model_cls.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])  # TODO:\n",
    "            model_cls.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        elif self.algorithm_name == \"cnn\":\n",
    "            # input_shape = (rows行, cols列, 1) 1表示颜色通道数目, rows行，对应一句话的长度, cols列表示词向量的维度\n",
    "            model_cls.add(Conv1D(64, 3, activation=\"relu\", input_shape=input_shape))  # filters, kernel_size\n",
    "            model_cls.add(Conv1D(64, 3, activation=\"relu\"))\n",
    "            model_cls.add(MaxPooling1D(3))\n",
    "            model_cls.add(Conv1D(128, 3, activation=\"relu\"))\n",
    "            model_cls.add(Conv1D(128, 3, activation=\"relu\"))\n",
    "            model_cls.add(GlobalAveragePooling1D())\n",
    "            model_cls.add(Dropout(0.25))\n",
    "            model_cls.add(Dense(2, activation=\"sigmoid\"))\n",
    "\n",
    "            model_cls.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])  # TODO: categorical_crossentropy\n",
    "        elif self.algorithm_name == \"lstm\":\n",
    "            model_cls.add(Masking(mask_value=-1, input_shape=input_shape, name=\"masking_layer\"))\n",
    "            model_cls.add(LSTM(units=64, return_sequences=True, dropout=0.25, name=\"lstm1\"))\n",
    "            model_cls.add(LSTM(units=128, return_sequences=False, dropout=0.25, name=\"lstm2\"))\n",
    "            model_cls.add(Dense(units=2, activation=\"softmax\", name=\"dense5\"))\n",
    "\n",
    "            model_cls.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "        return model_cls\n",
    "\n",
    "    def model_train(self, model_cls, X_train, X_val, y_train, y_val):\n",
    "        \"\"\"\n",
    "        分类器模型的训练\n",
    "        :param model_cls: 所使用的算法的类的定义，尚未训练的模型\n",
    "        :param X_train: \n",
    "        :param y_train: \n",
    "        :param X_val: \n",
    "        :param y_val: \n",
    "        :return: 训练好的模型\n",
    "        \"\"\"\n",
    "        early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        lr_reduction = ReduceLROnPlateau(monitor=\"val_loss\", patience=5, verbose=1, factor=0.2, min_lr=1e-5)\n",
    "        # 检查最好模型: 只要有提升, 就保存一次. 保存到多个模型文件\n",
    "        # model_path = f\"../data/output/models/{self.algorithm_name}_best_model_{epoch:02d}_{val_loss:.2f}.hdf5\"  # NO\n",
    "        model_path = \"../data/output/models/best_model_{epoch:02d}_{val_loss:.2f}.hdf5\"  # OK\n",
    "        checkpoint = ModelCheckpoint(filepath=model_path, monitor=\"val_loss\", verbose=1, save_best_only=True,\n",
    "                                     mode=\"min\")\n",
    "\n",
    "        hist_obj = model_cls.fit(X_train, y_train, batch_size=self.bath_size, epochs=self.epochs, verbose=1,\n",
    "                                 validation_data=(X_val, y_val), callbacks=[early_stopping, lr_reduction, checkpoint])\n",
    "        with open(\"../data/output/history_\"+self.algorithm_name+\"_\"+self.sent_vec_type+\".pkl\", \"wb\") as f:\n",
    "            pickle.dump(hist_obj.history, f)\n",
    "        return model_cls  # model\n",
    "\n",
    "    def plot_hist(self, history_filename):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        history = None\n",
    "        with open(\"../data/output/\"+history_filename+\".pkl\", \"rb\") as f:\n",
    "            history = pickle.load(f)\n",
    "\n",
    "        if not history:\n",
    "            return\n",
    "        # 绘制训练集和验证集的曲线\n",
    "        plt.plot(history[\"acc\"], label=\"Training Accuracy\", color=\"green\", linewidth=1)\n",
    "        plt.plot(history[\"loss\"], label=\"Training Loss\", color=\"red\", linewidth=1)\n",
    "        plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\", color=\"purple\", linewidth=1)\n",
    "        plt.plot(history[\"val_loss\"], label=\"Validation Loss\", color=\"blue\", linewidth=1)\n",
    "        plt.grid(True)  # 设置网格形式\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"acc-loss\")  # 给x, y轴加注释\n",
    "        plt.legend(loc=\"upper right\")  # 设置图例显示位置\n",
    "        plt.show()\n",
    "\n",
    "    def model_evaluate(self, model, X_val, y_val):\n",
    "        \"\"\"\n",
    "        分类器模型的评估\n",
    "        :param model: 训练好的模型对象\n",
    "        :param X_val: \n",
    "        :param y_val: \n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"model.metrics:{0}, model.metrics_names:{1}\".format(model.metrics, model.metrics_names))\n",
    "        scores = model.evaluate(X_val, y_val)\n",
    "        loss, accuracy = scores[0], scores[1] * 100\n",
    "        print(\"Loss: \")\n",
    "        print(loss)\n",
    "        print(\"\\n\"+self.algorithm_name+\" Classification Accuracy:\")\n",
    "        print(accuracy)\n",
    "\n",
    "    def model_predict(self, model):\n",
    "        \"\"\"\n",
    "        模型测试\n",
    "        :param model: 训练好的模型对象\n",
    "        :param preprocess_obj: Preprocessing类对象\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        sentence = \"这件 衣服 真的 太 好看 了 ！ 好想 买 啊 \"  # TODO:\n",
    "        sent_vec = np.array(self.preprocess_obj.gen_sentence_vec(sentence))  # shape: (70, 200)\n",
    "        if self.sent_vec_type == \"matrix\":  # cnn or lstm\n",
    "            sent_vec = sent_vec.reshape(1, sent_vec.shape[0], sent_vec.shape[1])\n",
    "        elif self.algorithm_name == \"nn\":  # nn.\n",
    "            sent_vec = sent_vec.reshape(1, sent_vec.shape[0])\n",
    "        elif self.algorithm_name == \"lstm\" and (self.sent_vec_type == \"avg\" or self.sent_vec_type == \"fasttext\"):  # lstm.\n",
    "            sent_vec = sent_vec.reshape(1, 1, sent_vec.shape[0])\n",
    "        print(\"'{sentence}': {np.argmax(model.predict(sent_vec))}\")  # 0: 正向\n",
    "\n",
    "        sentence = \"这 真的是 一部 非常 优秀 电影 作品\"\n",
    "        sent_vec = np.array(self.preprocess_obj.gen_sentence_vec(sentence))\n",
    "        if self.sent_vec_type == \"matrix\":  # cnn or lstm\n",
    "            sent_vec = sent_vec.reshape(1, sent_vec.shape[0], sent_vec.shape[1])\n",
    "        elif self.algorithm_name == \"nn\":  # nn.\n",
    "            sent_vec = sent_vec.reshape(1, sent_vec.shape[0])\n",
    "        elif self.algorithm_name == \"lstm\" and (self.sent_vec_type == \"avg\" or self.sent_vec_type == \"fasttext\"):  # lstm.\n",
    "            sent_vec = sent_vec.reshape(1, 1, sent_vec.shape[0])\n",
    "        print(\"'{sentence}': {np.argmax(model.predict(sent_vec))}\")  # 0: 正向\n",
    "\n",
    "        sentence = \"这个 电视 真 尼玛 垃圾 ， 老子 再也 不买 了\"\n",
    "        sent_vec = np.array(self.preprocess_obj.gen_sentence_vec(sentence))\n",
    "        if self.sent_vec_type == \"matrix\":  # cnn or lstm\n",
    "            sent_vec = sent_vec.reshape(1, sent_vec.shape[0], sent_vec.shape[1])\n",
    "        elif self.algorithm_name == \"nn\":  # nn.\n",
    "            sent_vec = sent_vec.reshape(1, sent_vec.shape[0])\n",
    "        elif self.algorithm_name == \"lstm\" and (self.sent_vec_type == \"avg\" or self.sent_vec_type == \"fasttext\"):  # lstm.\n",
    "            sent_vec = sent_vec.reshape(1, 1, sent_vec.shape[0])\n",
    "        print(\"'{sentence}': {np.argmax(model.predict(sent_vec))}\")  # 1: 负向\n",
    "\n",
    "        sentence_df = pd.read_csv(\"../data/input/training_set.txt\", sep=\"\\t\", header=None, names=[\"label\", \"sentence\"])\n",
    "        sentence_df = sentence_df.sample(frac=1)\n",
    "        sentence_series = sentence_df[\"sentence\"]\n",
    "        label_series = sentence_df[\"label\"]\n",
    "        print(\"label_series: {label_series.iloc[:11]}\")\n",
    "        count = 0\n",
    "        for sentence in sentence_series:\n",
    "            count += 1\n",
    "            sentence = sentence.strip()\n",
    "            sent_vec = np.array(self.preprocess_obj.gen_sentence_vec(sentence))\n",
    "            if self.sent_vec_type == \"matrix\":  # cnn or lstm\n",
    "                sent_vec = sent_vec.reshape(1, sent_vec.shape[0], sent_vec.shape[1])\n",
    "            elif self.algorithm_name == \"nn\":  # nn.\n",
    "                sent_vec = sent_vec.reshape(1, sent_vec.shape[0])\n",
    "            elif self.algorithm_name == \"lstm\" and (self.sent_vec_type == \"avg\" or self.sent_vec_type == \"fasttext\"):  # lstm.\n",
    "                sent_vec = sent_vec.reshape(1, 1, sent_vec.shape[0])\n",
    "            print(\"'{sentence}': {np.argmax(model.predict(sent_vec))}\")  # 0: 正向, 1: 负向\n",
    "            if count > 10:\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fasttext+lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:101: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:102: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11702 samples, validate on 3911 samples\n",
      "Epoch 1/1000\n",
      "11702/11702 [==============================] - 3s 279us/step - loss: 0.6926 - acc: 0.5185 - val_loss: 0.6924 - val_acc: 0.5142\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69244, saving model to ../data/output/models/best_model_01_0.69.hdf5\n",
      "Epoch 2/1000\n",
      "11702/11702 [==============================] - 1s 104us/step - loss: 0.6900 - acc: 0.5339 - val_loss: 0.6911 - val_acc: 0.5150\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69244 to 0.69111, saving model to ../data/output/models/best_model_02_0.69.hdf5\n",
      "Epoch 3/1000\n",
      "11702/11702 [==============================] - 1s 108us/step - loss: 0.6864 - acc: 0.5420 - val_loss: 0.6912 - val_acc: 0.5193\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69111\n",
      "Epoch 4/1000\n",
      "11702/11702 [==============================] - 1s 112us/step - loss: 0.6830 - acc: 0.5451 - val_loss: 0.6917 - val_acc: 0.5214\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69111\n",
      "Epoch 5/1000\n",
      "11702/11702 [==============================] - 1s 93us/step - loss: 0.6819 - acc: 0.5465 - val_loss: 0.6920 - val_acc: 0.5214\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69111\n",
      "Epoch 6/1000\n",
      "11702/11702 [==============================] - 1s 84us/step - loss: 0.6817 - acc: 0.5470 - val_loss: 0.6921 - val_acc: 0.5219\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69111\n",
      "Epoch 7/1000\n",
      "11702/11702 [==============================] - 1s 116us/step - loss: 0.6803 - acc: 0.5473 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69111\n",
      "Epoch 8/1000\n",
      "11702/11702 [==============================] - 1s 77us/step - loss: 0.6809 - acc: 0.5469 - val_loss: 0.6921 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69111\n",
      "Epoch 9/1000\n",
      "11702/11702 [==============================] - 1s 69us/step - loss: 0.6812 - acc: 0.5457 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69111\n",
      "Epoch 10/1000\n",
      "11702/11702 [==============================] - 1s 97us/step - loss: 0.6804 - acc: 0.5473 - val_loss: 0.6920 - val_acc: 0.5226\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69111\n",
      "Epoch 11/1000\n",
      "11702/11702 [==============================] - 1s 98us/step - loss: 0.6803 - acc: 0.5467 - val_loss: 0.6921 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.69111\n",
      "Epoch 12/1000\n",
      "11702/11702 [==============================] - 1s 122us/step - loss: 0.6797 - acc: 0.5472 - val_loss: 0.6921 - val_acc: 0.5221\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.69111\n",
      "model.metrics:['accuracy'], model.metrics_names:['loss', 'acc']\n",
      "3911/3911 [==============================] - 1s 168us/step\n",
      "Loss: \n",
      "0.6921460853455713\n",
      "\n",
      "lstm Classification Accuracy:\n",
      "52.21171056453117\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWCAxxhgTlP8Px7Wgnjm5APcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Program Running Cost {end_time -start_time:.2f}s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.set_sent_vec_type(\"fasttext\")\n",
    "X_train, X_val, y_train, y_val = preprocess_obj.gen_train_val_data()\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "sent_analyse = SentimentAnalysis(preprocess_obj, \"fasttext\")\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "sent_analyse.pick_algorithm(\"lstm\", \"fasttext\")\n",
    "model_cls = sent_analyse.model_build(input_shape=input_shape)\n",
    "model = sent_analyse.model_train(model_cls, X_train, X_val, y_train, y_val)\n",
    "sent_analyse.model_evaluate(model, X_val, y_val)\n",
    "sent_analyse.plot_hist(\"history_lstm_fasttext\")\n",
    "'''\n",
    "\n",
    "    sent_vec_type_list = [\"avg\", \"fasttext\", \"matrix\"]  # NN(MLP): 只能使用avg或fasttext. CNN: 只能使用matrix. LSTM: avg, fasttext, matrix均可.\n",
    "    sent_vec_type = sent_vec_type_list[0]\n",
    "    print(f\"\\n{sent_vec_type} and\", end=\" \")\n",
    "    preprocess_obj.set_sent_vec_type(sent_vec_type)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = preprocess_obj.gen_train_val_data()\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    y_val = np_utils.to_categorical(y_val)\n",
    "\n",
    "    sent_analyse = SentimentAnalysis(preprocess_obj, sent_vec_type)\n",
    "    algorithm_list = [\"nn\", \"cnn\", \"lstm\"]\n",
    "    algorithm_name = algorithm_list[0]\n",
    "\n",
    "    if len(X_train.shape) == 2 and algorithm_name == \"lstm\":  # avg or fasttext\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "        X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    elif algorithm_name == \"nn\":\n",
    "        input_shape = (X_train.shape[1],)\n",
    "    else:  # algorithm_name == \"cnn\" or (algorithm_name == \"lstm\" and len(X_train.shape) == 3)\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    print(X_train.shape, y_train.shape)  # (19998, 200)/(19998, 70, 200) (19998,)\n",
    "    print(X_val.shape, y_val.shape)  # (5998, 200)/(5998, 70, 200) (5998,)\n",
    "\n",
    "    print(f\"{algorithm_name}:\")\n",
    "    sent_analyse.pick_algorithm(algorithm_name, sent_vec_type)\n",
    "    # \"\"\"\n",
    "    model_cls = sent_analyse.model_build(input_shape=input_shape)\n",
    "    model = sent_analyse.model_train(model_cls, X_train, X_val, y_train, y_val)\n",
    "    # \"\"\"\n",
    "    sent_analyse.model_evaluate(model, X_val, y_val)\n",
    "    sent_analyse.model_predict(model)\n",
    "'''\n",
    "end_time = time.time()\n",
    "print(\"\\nProgram Running Cost {end_time -start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:101: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:102: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.set_sent_vec_type(\"fasttext\")\n",
    "X_train, X_test, y_train, y_test = preprocess_obj.gen_train_val_data()\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11702 samples, validate on 3911 samples\n",
      "Epoch 1/1000\n",
      "11702/11702 [==============================] - 43s 4ms/step - loss: 0.6932 - acc: 0.5032 - val_loss: 0.6932 - val_acc: 0.4986\n",
      "Epoch 2/1000\n",
      "11702/11702 [==============================] - 37s 3ms/step - loss: 0.6928 - acc: 0.5014 - val_loss: 0.6925 - val_acc: 0.5132\n",
      "Epoch 3/1000\n",
      "11702/11702 [==============================] - 34s 3ms/step - loss: 0.6910 - acc: 0.5236 - val_loss: 0.6921 - val_acc: 0.5134\n",
      "Epoch 4/1000\n",
      "11702/11702 [==============================] - 27s 2ms/step - loss: 0.6910 - acc: 0.5094 - val_loss: 0.6920 - val_acc: 0.5129\n",
      "Epoch 5/1000\n",
      " 9216/11702 [======================>.......] - ETA: 5s - loss: 0.6900 - acc: 0.5241"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6e0102dc2878>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlr_reduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhist_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_reduction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2667\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2649\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model_cls = Sequential()\n",
    "model_cls.add(Masking(mask_value=-1, input_shape=input_shape, name=\"masking_layer\"))\n",
    "model_cls.add(LSTM(units=64, return_sequences=True, dropout=0.25, name=\"lstm1\"))\n",
    "model_cls.add(LSTM(units=128, return_sequences=False, dropout=0.25, name=\"lstm2\"))\n",
    "model_cls.add(Dense(units=2, activation=\"softmax\", name=\"dense5\"))\n",
    "model_cls.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "lr_reduction = ReduceLROnPlateau(monitor=\"val_loss\", patience=5, verbose=1, factor=0.2, min_lr=1e-5)\n",
    "hist_obj = model_cls.fit(X_train, y_train, batch_size=512, epochs=1000, verbose=1,validation_data=(X_test, y_test), callbacks=[early_stopping, lr_reduction])\n",
    "\n",
    "score = model_cls.evaluate(X_test, y_test, batch_size = 512)\n",
    "print ('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11702 samples, validate on 3911 samples\n",
      "Epoch 1/1000\n",
      "11702/11702 [==============================] - 6s 530us/step - loss: 0.6922 - acc: 0.5067 - val_loss: 0.6922 - val_acc: 0.5180\n",
      "Epoch 2/1000\n",
      "11702/11702 [==============================] - 3s 290us/step - loss: 0.6910 - acc: 0.5297 - val_loss: 0.6920 - val_acc: 0.5180\n",
      "Epoch 3/1000\n",
      "11702/11702 [==============================] - 3s 293us/step - loss: 0.6903 - acc: 0.5297 - val_loss: 0.6929 - val_acc: 0.5180\n",
      "Epoch 4/1000\n",
      "11702/11702 [==============================] - 3s 296us/step - loss: 0.6895 - acc: 0.5274 - val_loss: 0.6915 - val_acc: 0.5216\n",
      "Epoch 5/1000\n",
      "11702/11702 [==============================] - 3s 287us/step - loss: 0.6882 - acc: 0.5342 - val_loss: 0.6920 - val_acc: 0.5234\n",
      "Epoch 6/1000\n",
      "11702/11702 [==============================] - 3s 288us/step - loss: 0.6866 - acc: 0.5402 - val_loss: 0.6923 - val_acc: 0.5238\n",
      "Epoch 7/1000\n",
      "11702/11702 [==============================] - 3s 287us/step - loss: 0.6869 - acc: 0.5385 - val_loss: 0.6928 - val_acc: 0.5238\n",
      "Epoch 8/1000\n",
      "11702/11702 [==============================] - 3s 286us/step - loss: 0.6860 - acc: 0.5406 - val_loss: 0.6922 - val_acc: 0.5148\n",
      "Epoch 9/1000\n",
      "11702/11702 [==============================] - 3s 287us/step - loss: 0.6867 - acc: 0.5392 - val_loss: 0.6948 - val_acc: 0.5237\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/1000\n",
      "11702/11702 [==============================] - 3s 283us/step - loss: 0.6854 - acc: 0.5417 - val_loss: 0.6914 - val_acc: 0.5198\n",
      "Epoch 11/1000\n",
      "11702/11702 [==============================] - 3s 285us/step - loss: 0.6841 - acc: 0.5438 - val_loss: 0.6926 - val_acc: 0.5208\n",
      "Epoch 12/1000\n",
      "11702/11702 [==============================] - 3s 284us/step - loss: 0.6838 - acc: 0.5428 - val_loss: 0.6931 - val_acc: 0.5208\n",
      "Epoch 13/1000\n",
      "11702/11702 [==============================] - 3s 297us/step - loss: 0.6834 - acc: 0.5437 - val_loss: 0.6931 - val_acc: 0.5214\n",
      "Epoch 14/1000\n",
      "11702/11702 [==============================] - 4s 337us/step - loss: 0.6838 - acc: 0.5429 - val_loss: 0.6933 - val_acc: 0.5211\n",
      "Epoch 15/1000\n",
      "11702/11702 [==============================] - 3s 287us/step - loss: 0.6835 - acc: 0.5447 - val_loss: 0.6924 - val_acc: 0.5198\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 16/1000\n",
      "11702/11702 [==============================] - 3s 285us/step - loss: 0.6835 - acc: 0.5444 - val_loss: 0.6926 - val_acc: 0.5215\n",
      "Epoch 17/1000\n",
      "11702/11702 [==============================] - 3s 288us/step - loss: 0.6830 - acc: 0.5435 - val_loss: 0.6925 - val_acc: 0.5216\n",
      "Epoch 18/1000\n",
      "11702/11702 [==============================] - 3s 288us/step - loss: 0.6828 - acc: 0.5448 - val_loss: 0.6927 - val_acc: 0.5216\n",
      "Epoch 19/1000\n",
      "11702/11702 [==============================] - 3s 288us/step - loss: 0.6828 - acc: 0.5447 - val_loss: 0.6927 - val_acc: 0.5216\n",
      "Epoch 20/1000\n",
      "11702/11702 [==============================] - 3s 285us/step - loss: 0.6828 - acc: 0.5448 - val_loss: 0.6928 - val_acc: 0.5216\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "3911/3911 [==============================] - 0s 80us/step\n",
      "Test accuracy:  0.5216057288680227\n"
     ]
    }
   ],
   "source": [
    "model_cls = Sequential()\n",
    "model_cls.add(Conv1D(64, 3, activation=\"relu\", input_shape=(X_train.shape[1:])))  # filters, kernel_size\n",
    "model_cls.add(Conv1D(64, 3, activation=\"relu\"))\n",
    "model_cls.add(MaxPooling1D(3))\n",
    "model_cls.add(Conv1D(128, 3, activation=\"relu\"))\n",
    "model_cls.add(Conv1D(128, 3, activation=\"relu\"))\n",
    "model_cls.add(GlobalAveragePooling1D())\n",
    "model_cls.add(Dropout(0.25))\n",
    "model_cls.add(Dense(2, activation=\"sigmoid\"))\n",
    "model_cls.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])  # TODO: categorical_crossentropy\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "lr_reduction = ReduceLROnPlateau(monitor=\"val_loss\", patience=5, verbose=1, factor=0.2, min_lr=1e-5)\n",
    "hist_obj = model_cls.fit(X_train, y_train, batch_size=512, epochs=1000, verbose=1,validation_data=(X_test, y_test), callbacks=[early_stopping, lr_reduction])\n",
    "\n",
    "score = model_cls.evaluate(X_test, y_test, batch_size = 512)\n",
    "print ('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:101: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:102: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, kernel_initializer=\"uniform\", input_dim=50, activation=\"tanh\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11702/11702 [==============================] - 12s 1ms/step - loss: 0.6905 - acc: 0.5172\n",
      "Epoch 2/20\n",
      "11702/11702 [==============================] - 10s 871us/step - loss: 0.6901 - acc: 0.5232\n",
      "Epoch 3/20\n",
      "11702/11702 [==============================] - 10s 859us/step - loss: 0.6890 - acc: 0.5345\n",
      "Epoch 4/20\n",
      "11702/11702 [==============================] - 10s 860us/step - loss: 0.6872 - acc: 0.5288\n",
      "Epoch 5/20\n",
      "11702/11702 [==============================] - 10s 852us/step - loss: 0.6871 - acc: 0.5404\n",
      "Epoch 6/20\n",
      "11702/11702 [==============================] - 10s 854us/step - loss: 0.6852 - acc: 0.5390\n",
      "Epoch 7/20\n",
      "11702/11702 [==============================] - 10s 833us/step - loss: 0.6854 - acc: 0.5461\n",
      "Epoch 8/20\n",
      "11702/11702 [==============================] - 11s 904us/step - loss: 0.6835 - acc: 0.5443\n",
      "Epoch 9/20\n",
      "11702/11702 [==============================] - 10s 864us/step - loss: 0.6847 - acc: 0.5428\n",
      "Epoch 10/20\n",
      "11702/11702 [==============================] - 10s 829us/step - loss: 0.6843 - acc: 0.5449\n",
      "Epoch 11/20\n",
      "11702/11702 [==============================] - 10s 830us/step - loss: 0.6827 - acc: 0.5467\n",
      "Epoch 12/20\n",
      "11702/11702 [==============================] - 10s 889us/step - loss: 0.6855 - acc: 0.5429\n",
      "Epoch 13/20\n",
      "11702/11702 [==============================] - 10s 837us/step - loss: 0.6823 - acc: 0.5462\n",
      "Epoch 14/20\n",
      "11702/11702 [==============================] - 10s 851us/step - loss: 0.6830 - acc: 0.5437\n",
      "Epoch 15/20\n",
      "11702/11702 [==============================] - 10s 856us/step - loss: 0.6831 - acc: 0.5436\n",
      "Epoch 16/20\n",
      "11702/11702 [==============================] - 10s 859us/step - loss: 0.6845 - acc: 0.5392\n",
      "Epoch 17/20\n",
      "11702/11702 [==============================] - 10s 849us/step - loss: 0.6834 - acc: 0.5466\n",
      "Epoch 18/20\n",
      "11702/11702 [==============================] - 10s 843us/step - loss: 0.6859 - acc: 0.5445\n",
      "Epoch 19/20\n",
      "11702/11702 [==============================] - 10s 864us/step - loss: 0.6833 - acc: 0.5455\n",
      "Epoch 20/20\n",
      "11702/11702 [==============================] - 10s 887us/step - loss: 0.6835 - acc: 0.5467\n",
      "3911/3911 [==============================] - 2s 466us/step\n",
      "Test accuracy:  0.5231398619736165\n"
     ]
    }
   ],
   "source": [
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.set_sent_vec_type(\"fasttext\")\n",
    "X_train, X_test, y_train, y_test = preprocess_obj.gen_train_val_data()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = 50, init = 'uniform', activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, nb_epoch = 20, batch_size = 16)\n",
    "score = model.evaluate(X_test, y_test, batch_size = 16)\n",
    "print ('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(512, kernel_initializer=\"uniform\", input_dim=50, activation=\"tanh\")`\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:18: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "11702/11702 [==============================] - 13s 1ms/step - loss: 0.6925 - acc: 0.5146\n",
      "Epoch 2/20\n",
      "11702/11702 [==============================] - 10s 860us/step - loss: 0.6911 - acc: 0.5270\n",
      "Epoch 3/20\n",
      "11702/11702 [==============================] - 9s 797us/step - loss: 0.6879 - acc: 0.5321\n",
      "Epoch 4/20\n",
      "11702/11702 [==============================] - 10s 845us/step - loss: 0.6864 - acc: 0.5333\n",
      "Epoch 5/20\n",
      "11702/11702 [==============================] - 10s 839us/step - loss: 0.6858 - acc: 0.5335\n",
      "Epoch 6/20\n",
      "11702/11702 [==============================] - 10s 895us/step - loss: 0.6864 - acc: 0.5421\n",
      "Epoch 7/20\n",
      "11702/11702 [==============================] - 10s 849us/step - loss: 0.6856 - acc: 0.5391\n",
      "Epoch 8/20\n",
      "11702/11702 [==============================] - 10s 872us/step - loss: 0.6856 - acc: 0.5426\n",
      "Epoch 9/20\n",
      "11702/11702 [==============================] - 10s 856us/step - loss: 0.6847 - acc: 0.5412\n",
      "Epoch 10/20\n",
      "11702/11702 [==============================] - 10s 849us/step - loss: 0.6841 - acc: 0.5429\n",
      "Epoch 11/20\n",
      "11702/11702 [==============================] - 10s 837us/step - loss: 0.6855 - acc: 0.5452\n",
      "Epoch 12/20\n",
      "11702/11702 [==============================] - 10s 842us/step - loss: 0.6830 - acc: 0.5433\n",
      "Epoch 13/20\n",
      "11702/11702 [==============================] - 10s 871us/step - loss: 0.6847 - acc: 0.5430\n",
      "Epoch 14/20\n",
      "11702/11702 [==============================] - 9s 806us/step - loss: 0.6835 - acc: 0.5428\n",
      "Epoch 15/20\n",
      "11702/11702 [==============================] - 10s 886us/step - loss: 0.6853 - acc: 0.5396\n",
      "Epoch 16/20\n",
      "11702/11702 [==============================] - 10s 848us/step - loss: 0.6845 - acc: 0.5442\n",
      "Epoch 17/20\n",
      "11702/11702 [==============================] - 10s 838us/step - loss: 0.6837 - acc: 0.5461\n",
      "Epoch 18/20\n",
      "11702/11702 [==============================] - 10s 857us/step - loss: 0.6839 - acc: 0.5431\n",
      "Epoch 19/20\n",
      "11702/11702 [==============================] - ETA: 0s - loss: 0.6829 - acc: 0.543 - 10s 847us/step - loss: 0.6828 - acc: 0.5435\n",
      "Epoch 20/20\n",
      "11702/11702 [==============================] - 10s 841us/step - loss: 0.6837 - acc: 0.5393\n",
      "3911/3911 [==============================] - 1s 378us/step\n",
      "Test accuracy:  0.5233955510556927\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_dim = 50, init = 'uniform', activation = 'tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, nb_epoch = 20, batch_size = 16)\n",
    "score = model.evaluate(X_test, y_test, batch_size = 16)\n",
    "print ('Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:101: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:102: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM= 0.5205829711071337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "preprocess_obj.set_sent_vec_type(\"fasttext\")\n",
    "train_set1, test_set1, target_train, target_test = preprocess_obj.gen_train_val_data()\n",
    "svclf = SVC(kernel ='linear',probability=True)\n",
    "svclf.fit(train_set1, target_train)\n",
    "pred_svc = svclf.predict(test_set1)\n",
    "print('SVM=',sum(pred_svc==target_test)/len(target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3911, 50)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:101: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:102: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM= 0.5236512400920481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "preprocess_obj = Preprocessing()\n",
    "preprocess_obj.set_sent_vec_type(\"fasttext\")\n",
    "train_set1, test_set1, target_train, target_test = preprocess_obj.gen_train_val_data()\n",
    "svclf = SVC(kernel ='linear',probability=True)\n",
    "svclf.fit(train_set1, target_train)\n",
    "pred_svc = svclf.predict(test_set1)\n",
    "print('SVM=',sum(pred_svc==target_test)/len(target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
