{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "import time\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "from pyfasttext import FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self):\n",
    "        start_time = time.time()\n",
    "        # self.model = FastText(\"../data/input/models/sg_pyfasttext.bin\")  # DEBUG\n",
    "        # self.model = FastText(\"../data/input/models/880w_fasttext_skip_gram.bin\")\n",
    "        # end_time = time.time()\n",
    "        # print(\"Loading word vector model cost: {end_time - start_time:.2f}s\")\n",
    "\n",
    "        # self.vocab_size, self.vector_size = self.model.numpy_normalized_vectors.shape  # OK\n",
    "        # self.vocab_size = self.model.nwords\n",
    "        # self.vector_size = self.model.args.get(\"dim\")\n",
    "        # self.vector_size:200, self.vocab_size: 925242\n",
    "        # print(\"self.vector_size:{self.vector_size}, self.vocab_size: {self.vocab_size}\")\n",
    "\n",
    "        # 句子的表示形式:\n",
    "        # {\"avg\": 向量和的平均, \"fasttext\": get_numpy_sentence_vector, \"concatenate\": 向量拼接和补齐, \"matrix\": 矩阵}\n",
    "        self.sentence_vec_type = \"matrix\"\n",
    "\n",
    "        self.MAX_SENT_LEN = 70  # DEBUG: 超参数. self.get_sent_max_length()\n",
    "        # 对于\"concatenate\": self.MAX_SENT_LEN = 30, 取其他不同值的结果: 100: 50.22%, 80: 50.23%, 70: 50.33%, 60: 55.92%, 50: 69.11%, 40: 68.91%, 36: 69.34%, 30: 69.22%, 20: 69.17%, 10: 67.07%\n",
    "        # 对于\"matrix\": self.MAX_SENT_LEN = 70, 取其他不同值的结果: TODO:\n",
    "\n",
    "    @classmethod\n",
    "    def data_analysis(cls):\n",
    "        train_df = pd.read_csv(\"../data/input/training_set.txt\", sep=\"\\t\", header=None, names=[\"label\", \"sentence\"])\n",
    "        val_df = pd.read_csv(\"../data/input/validation_set.txt\", sep=\"\\t\", header=None, names=[\"label\", \"sentence\"])\n",
    "        y_train = train_df[\"label\"]\n",
    "        y_val = val_df[\"label\"]\n",
    "        sns.set(style=\"white\", context=\"notebook\", palette=\"deep\")\n",
    "        # 查看样本数据分布情况(各个label数据是否均匀分布)\n",
    "        sns.countplot(y_train)\n",
    "        plt.show()\n",
    "        sns.countplot(y_val)\n",
    "        plt.show()\n",
    "        print(y_train.value_counts())\n",
    "        print(y_val.value_counts())\n",
    "\n",
    "    def set_sent_vec_type(self, sentence_vec_type):\n",
    "        assert sentence_vec_type in [\"avg\", \"concatenate\", \"fasttext\", \"matrix\"], \\\n",
    "            \"sentence_vec_type must be in ['avg', 'fasttext', 'concatenate', 'matrix']\"\n",
    "        self.sentence_vec_type = sentence_vec_type\n",
    "\n",
    "    def get_sent_max_length(self):  # NOT_USED\n",
    "        sent_len_counter = Counter()\n",
    "        max_length = 0\n",
    "        with open(\"../data/input/training_set.txt\") as f:\n",
    "            for line in f:\n",
    "                content = line.strip().split(\"\\t\")[1]\n",
    "                content_list = content.split()\n",
    "                length = len(content_list)\n",
    "                sent_len_counter[length] += 1\n",
    "                if max_length <= length:\n",
    "                    max_length = length\n",
    "        sent_len_counter = sorted(list(sent_len_counter.items()), key=lambda x: x[0])\n",
    "        print(sent_len_counter)\n",
    "        # [(31, 1145), (32, 1105), (33, 1017), (34, 938), (35, 839), (36, 830), (37, 775), (38, 737), (39, 720), (40, 643), (41, 575), (42, 584), (43, 517), (44, 547), (45, 514), (46, 514), (47, 480), (48, 460), (49, 470), (50, 444), (51, 484), (52, 432), (53, 462), (54, 495), (55, 487), (56, 500), (57, 496), (58, 489), (59, 419), (60, 387), (61, 348), (62, 265), (63, 222), (64, 153), (65, 127), (66, 103), (67, 67), (68, 34), (69, 21), (70, 22), (71, 8), (72, 6), (73, 4), (74, 10), (75, 2), (76, 4), (77, 2), (78, 1), (79, 2), (80, 4), (81, 2), (82, 3), (83, 1), (84, 5), (86, 4), (87, 3), (88, 3), (89, 2), (90, 2), (91, 3), (92, 5), (93, 2), (94, 4), (96, 1), (97, 5), (98, 1), (99, 2), (100, 2), (101, 2), (102, 1), (103, 2), (104, 2), (105, 2), (106, 5), (107, 3), (108, 2), (109, 3), (110, 4), (111, 1), (112, 2), (113, 3), (114, 1), (116, 1), (119, 3), (679, 1)]\n",
    "        return max_length\n",
    "\n",
    "    def gen_sentence_vec(self, sentence):\n",
    "        \"\"\"\n",
    "        :param sentence: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        sentence = sentence.strip()\n",
    "        if self.sentence_vec_type == \"fasttext\":\n",
    "            return self.model.get_numpy_sentence_vector(sentence)\n",
    "\n",
    "        word_list = sentence.split(\" \")\n",
    "        if self.sentence_vec_type == \"concatenate\":\n",
    "            sentence_vector = self.model.get_numpy_vector(word_list[0])\n",
    "            for word in word_list[1:]:\n",
    "                sentence_vector = np.hstack((sentence_vector, self.model.get_numpy_vector(word)))\n",
    "            return sentence_vector  # NOTE: 对于concatenate情况, 每个句子的sentence_vector是不一样长的\n",
    "        if self.sentence_vec_type == \"matrix\":  # for Deep Learning.\n",
    "            sentence_matrix = []\n",
    "            for word in word_list[-self.MAX_SENT_LEN:]:  # NOTE: 截取后面的应该是要好些(参考https://github.com/lxw0109/SentimentClassification_UMICH_SI650/blob/master/src/LSTM_wo_pretrained_vector.py#L86)\n",
    "                sentence_matrix.append(self.model.get_numpy_vector(word))\n",
    "            length = len(sentence_matrix)\n",
    "            # 一定成立，因为上面做了切片截取\n",
    "            assert length <= self.MAX_SENT_LEN, \"CRITICAL ERROR: len(sentence_matrix) > self.MAX_SENT_LEN.\"\n",
    "            # 参数中的matrix类型为list of ndarray, 返回值的matrix是ndarray of ndarray\n",
    "            sentence_matrix = np.pad(sentence_matrix, pad_width=((0, self.MAX_SENT_LEN - length), (0, 0)),\n",
    "                                     mode=\"constant\", constant_values=-1)\n",
    "            return sentence_matrix\n",
    "        else:  # self.sentence_vec_type == \"avg\":\n",
    "            sentence_vector = np.zeros(self.vector_size)  # <ndarray>\n",
    "            # print(f\"type(sentence_vector): {type(sentence_vector)}\")\n",
    "            for idx, word in enumerate(word_list):\n",
    "                # print(f\"type(self.model.get_numpy_vector(word)): {type(self.model.get_numpy_vector(word))}\")  # <ndarray>\n",
    "                sentence_vector += self.model.get_numpy_vector(word)\n",
    "            return sentence_vector / len(word_list)\n",
    "\n",
    "    def gen_train_val_data(self):\n",
    "        # 构造训练数据 & 验证数据\n",
    "        train_df = pd.read_csv(\"../data/input/training_set.txt\", sep=\"\\t\", header=None, names=[\"label\", \"sentence\"])\n",
    "        val_df = pd.read_csv(\"../data/input/validation_set.txt\", sep=\"\\t\", header=None, names=[\"label\", \"sentence\"])\n",
    "        # 打乱训练集的顺序. TODO: 不打乱感觉训练出来的模型是有问题的?(好看那句总是预测结果是1？)\n",
    "        train_df = train_df.sample(frac=1, random_state=1)\n",
    "        # val_df = val_df.sample(frac=1, random_state=1)  # 验证集不用打乱\n",
    "\n",
    "        X_train = train_df[\"sentence\"]\n",
    "        X_train_vec = list()\n",
    "        for sentence in X_train:\n",
    "            sent_vector = self.gen_sentence_vec(sentence)\n",
    "            X_train_vec.append(sent_vector)\n",
    "        y_train = train_df[\"label\"]  # <Series>\n",
    "\n",
    "        X_val = val_df[\"sentence\"]\n",
    "        X_val_vec = list()\n",
    "        for sentence in X_val:\n",
    "            sent_vector = self.gen_sentence_vec(sentence)\n",
    "            X_val_vec.append(sent_vector)\n",
    "        y_val = val_df[\"label\"]  # <Series>\n",
    "\n",
    "        if self.sentence_vec_type == \"concatenate\":\n",
    "            # NOTE: 注意，这里的dtype是必须的，否则dtype默认值是\"int32\", 词向量所有的数值会被全部转换为0\n",
    "            X_train_vec = sequence.pad_sequences(X_train_vec, maxlen=self.MAX_SENT_LEN * self.vector_size, value=0,\n",
    "                                             dtype=np.float)\n",
    "            X_val_vec = sequence.pad_sequences(X_val_vec, maxlen=self.MAX_SENT_LEN * self.vector_size, value=0,\n",
    "                                           dtype=np.float)\n",
    "\n",
    "        return np.array(X_train_vec), np.array(X_val_vec), np.array(y_train), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Preprocessing' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5efff7c4b9c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_train_val_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-f07f09075a50>\u001b[0m in \u001b[0;36mgen_train_val_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mX_train_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0msent_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_sentence_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mX_train_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# <Series>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f07f09075a50>\u001b[0m in \u001b[0;36mgen_sentence_vec\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0msentence_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_SENT_LEN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# NOTE: 截取后面的应该是要好些(参考https://github.com/lxw0109/SentimentClassification_UMICH_SI650/blob/master/src/LSTM_wo_pretrained_vector.py#L86)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msentence_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_numpy_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# 一定成立，因为上面做了切片截取\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Preprocessing' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = preprocess_obj.gen_train_val_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
